# Задания по работе с Apache Spark

```
Результаты домашней работы загружайте к себе в git репозиторий. Создайте папки
DE-101/Module7/
...
И сохраняйте там результат.
```

Для выполнения заданий вам необходимо скачать данные о полётах авиакомпаний США за 2015 год с сайта Kaggle:
https://www.kaggle.com/usdot/flight-delays

В этих трёх файлах содержатся сведения об отменах и задержках рейсов. 

```
Формат сдачи заданий: 
Для каждой задачи загрузите один .py-скрипт с кодом + 1 скриншот с результатом выполнения кода (чтобы была видна итоговая таблица с данными).
Так мы сможем и посмотреть на результат, и сделать небольшое код-ревью
```

```
Примечание: постарайтесь использовать при решении именно функции PySpark, а не чистый SQL.
Так вы получите больше навыков в программировании.
Но если вдруг у вас не получается даже после долгих попыток - ничего страшного, используйте temp view и SQL.
Главное - результат :)
```

## Задача №1

Найдите топ-5 аэропортов США по количеству задержанных прибытий рейсов в них.

(Речь идёт об аэропортах как точках прибытия, а не отправления! То есть **куда** задерживались рейсы, а не **откуда**)

Итоговый DataFrame должен содержать 2 колонки - название аэропорта, число задержанных рейсов, которые летели в этот аэропорт.

## Задача №2

Назовите топ-3 авиакомпании по числу отмененных рейсов (в порядке убывания).

Итоговый DataFrame должен содержать 2 колонки - название авиакомпании, число отмененных рейсов.

## Задача №3
Назовите топ-5 штатов США по самому высокому среднему времени, которое проводят в полете **улетающие** из этих штатов пассажиры.

Например, пассажиры, вылетающие из штата Техас (неважно куда они при этом летят), в среднем проводят в полете 4,3 часов. 
Пассажиры, вылетающие из Аляски - в среднем проводят в полете 7.5 часов, и т.д.
Нужно найти 5 самых "долголетающих" штатов США.

Итоговый DataFrame должен содержать 2 колонки - название штата, среднее число часов в пути для рейсов, улетающих из этого штата.